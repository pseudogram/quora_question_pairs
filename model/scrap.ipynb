{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages...\n"
     ]
    }
   ],
   "source": [
    "print('Importing packages...')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.layers import Merge\n",
    "from keras.layers import TimeDistributed, Lambda\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "import h5py # Used to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-03e32a3067f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m tk.fit_on_texts(list(data.question1.values) +\n\u001b[0;32m---> 36\u001b[0;31m                  list(data.question2.values.astype(str)))\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'tokenizer.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dattlee/miniconda/envs/quora_question_pairs/lib/python2.7/site-packages/keras/preprocessing/text.pyc\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    175\u001b[0m                                                                      self.split)\n\u001b[1;32m    176\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Reading Data...')\n",
    "\n",
    "# Output directory\n",
    "train_loc = 'local' # floyd or local\n",
    "num_epochs = 1  # Originally used 200\n",
    "\n",
    "if train_loc == 'floyd':\n",
    "    output = 'output/'\n",
    "    data_loc = 'data/'\n",
    "else:\n",
    "    output = '../output_conv/'\n",
    "    data_loc = '../data/'\n",
    "\n",
    "# Import data\n",
    "csv_train = data_loc + 'train.csv'\n",
    "data = pd.read_csv(csv_train)\n",
    "y = data.is_duplicate.values\n",
    "\n",
    "\n",
    "# Tokenize text\n",
    "\n",
    "num_words = 200000  # The total number of word to incorperates in the search\n",
    "max_len = 40 # The maximum length of a sequence\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle # If in python 2 environment\n",
    "except:\n",
    "    import pickle # If in a pythnon 3 environment\n",
    "\n",
    "######################################################\n",
    "# When training in one go use below\n",
    "######################################################\n",
    "tk = text.Tokenizer(num_words=num_words)  # Create a tokenizer object\n",
    "\n",
    "tk.fit_on_texts(list(data.question1.values) +\n",
    "                 list(data.question2.values.astype(str)))\n",
    "pickle.dump(tk,open(output + 'tokenizer.p','wb'))\n",
    "\n",
    "######################################################\n",
    "# When using tokenizer made earlier, use below\n",
    "######################################################\n",
    "\n",
    "# tk = pickle.load(open('../data/tokenizer.p','rb'))\n",
    "\n",
    "######################################################\n",
    "\n",
    "word_index = tk.word_index\n",
    "\n",
    "x1 = tk.texts_to_sequences(data.question1.values)\n",
    "x1 = sequence.pad_sequences(x1,maxlen=max_len)\n",
    "\n",
    "x2 = tk.texts_to_sequences(data.question2.values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2,maxlen=max_len)\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating embeddings...')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(data_loc + 'glove.840B.300d.txt')\n",
    "unfound = []\n",
    "unfound_vals = []\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        unfound.append(word)\n",
    "        unfound_vals.append(values[1:])\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "print('Creating embedding matrix...')\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "max_features = 200000\n",
    "filter_length = 5\n",
    "nb_filter = 64\n",
    "pool_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-361240601ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m model3.add(Embedding(len(word_index) + 1,\n\u001b[0m\u001b[1;32m      8\u001b[0m                      \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                      \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_index' is not defined"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "\n",
    "# Question 1 - Embeddings -> Convolutional\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=40,\n",
    "                     trainable=False))\n",
    "model3.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "# model3.add(Dropout(0.2))\n",
    "\n",
    "# model3.add(Convolution1D(nb_filter=nb_filter,\n",
    "#                          filter_length=filter_length,\n",
    "#                          border_mode='valid',\n",
    "#                          activation='relu',\n",
    "#                          subsample_length=1))\n",
    "\n",
    "model3.add(GlobalMaxPooling1D())\n",
    "# model3.add(Dropout(0.2))\n",
    "\n",
    "# model3.add(Dense(300))\n",
    "# model3.add(Dropout(0.2))\n",
    "# model3.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "# Question 2 - Embeddings -> Convolutional\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=40,\n",
    "                     trainable=False))\n",
    "model4.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "# model4.add(Dropout(0.2))\n",
    "\n",
    "# model4.add(Convolution1D(nb_filter=nb_filter,\n",
    "#                          filter_length=filter_length,\n",
    "#                          border_mode='valid',\n",
    "#                          activation='relu',\n",
    "#                          subsample_length=1))\n",
    "\n",
    "model4.add(GlobalMaxPooling1D())\n",
    "# model4.add(Dropout(0.2))\n",
    "\n",
    "# model4.add(Dense(300))\n",
    "# model4.add(Dropout(0.2))\n",
    "# model4.add(BatchNormalization())\n",
    "\n",
    "# Merge all models - MERGE ALL QUESTIONS\n",
    "\n",
    "merged_model = Sequential()\n",
    "merged_model.add(Merge([model3, model4], mode='concat'))\n",
    "merged_model.add(BatchNormalization())\n",
    "\n",
    "# Feed Forward Network\n",
    "# merged_model.add(Dense(300))\n",
    "# merged_model.add(PReLU())\n",
    "# merged_model.add(Dropout(0.2))\n",
    "# merged_model.add(BatchNormalization())\n",
    "#\n",
    "# merged_model.add(Dense(300))\n",
    "# merged_model.add(PReLU())\n",
    "# merged_model.add(Dropout(0.2))\n",
    "# merged_model.add(BatchNormalization())\n",
    "#\n",
    "# merged_model.add(Dense(300))\n",
    "# merged_model.add(PReLU())\n",
    "# merged_model.add(Dropout(0.2))\n",
    "# merged_model.add(BatchNormalization())\n",
    "\n",
    "# Final node gives binary output\n",
    "merged_model.add(Dense(1))\n",
    "merged_model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2b6e254fb2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loss, Optimizer, Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmerged_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Save checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Loss, Optimizer, Accuracy\n",
    "merged_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 384\n",
    "# Save checkpoints\n",
    "checkpoint = ModelCheckpoint(output+'weights.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint_board = Tensorboard(log_dir=output+'/logs/',batch_size=batch_size)\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "checkpoint_3 = EarlyStopping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/1\n",
      "363648/363861 [============================>.] - ETA: 0s - loss: 0.5295 - acc: 0.7381Epoch 00000: val_acc improved from -inf to 0.75656, saving model to ../output/weights.h5\n",
      "363861/363861 [==============================] - 229s - loss: 0.5295 - acc: 0.7381 - val_loss: 0.4975 - val_acc: 0.7566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17f2901d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model on x1 and x2\n",
    "history = merged_model.fit([x1, x2], y=y, batch_size=batch_size, nb_epoch=num_epochs,\n",
    "                 verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint]) # Add tensorboard in callbacks list\n",
    "\n",
    "pickle.dump(tk,open(output + 'tokenizer.p','wb'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = merged_model.to_json()\n",
    "with open(output + \"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dattlee/miniconda/envs/quora_question_pairs/lib/python2.7/site-packages/keras/engine/topology.py:1252: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  return cls(**config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "404192/404290 [============================>.] - ETA: 0sacc: 80.81%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Load Model\n",
    "\n",
    "# load json and create model\n",
    "json_file = open(output+'model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(output+\"weights.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate([x1,x2], y, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "csv_test = data_loc + 'test.csv'\n",
    "test_data = pd.read_csv(csv_test)\n",
    "\n",
    "# # Tokenize text\n",
    "# num_words = 200000  # The total number of word to incorperates in the search\n",
    "# tk = text.Tokenizer(num_words=num_words)  # Create a tokenizer object\n",
    "\n",
    "# max_len = 40 # The maximum length of a sequence\n",
    "# tk.fit_on_texts(list(data.question1.values) +\n",
    "#                 list(data.question2.values.astype(str)))\n",
    "\n",
    "t_x1 = tk.texts_to_sequences(test_data.question1.values.astype(str))\n",
    "t_x1 = sequence.pad_sequences(t_x1,maxlen=max_len)\n",
    "\n",
    "t_x2 = tk.texts_to_sequences(test_data.question2.values.astype(str))\n",
    "t_x2 = sequence.pad_sequences(t_x2,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2345600/2345796 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "p_test = loaded_model.predict([t_x1,t_x2],batch_size=100,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "df_test = pd.read_csv(data_loc+'test.csv')\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv(output + 'shitty_conv.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
