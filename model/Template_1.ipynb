{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing packages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "print('Importing packages...')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.layers import Merge\n",
    "from keras.layers import TimeDistributed, Lambda\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.preprocessing import sequence, text\n",
    "\n",
    "import h5py # Used to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data...\n"
     ]
    }
   ],
   "source": [
    "print('Reading Data...')\n",
    "\n",
    "# Output directory\n",
    "train_loc = 'local' # floyd or local\n",
    "num_epochs = 1  # Originally used 200\n",
    "\n",
    "if train_loc == 'floyd':\n",
    "    output = '../output/'\n",
    "    data_loc = '../data/'\n",
    "else:\n",
    "    output = '../output_conv/'\n",
    "    data_loc = '../data/'\n",
    "\n",
    "# Import data\n",
    "csv_train = data_loc + 'train.csv'\n",
    "data = pd.read_csv(csv_train)\n",
    "y = data.is_duplicate.values\n",
    "\n",
    "\n",
    "# Tokenize text\n",
    "\n",
    "num_words = 200000  # The total number of word to incorperates in the search\n",
    "max_len = 40 # The maximum length of a sequence\n",
    "\n",
    "######################################################\n",
    "# When training in one go use below\n",
    "######################################################\n",
    "# tk = text.Tokenizer(num_words=num_words)  # Create a tokenizer object\n",
    "\n",
    "# tk.fit_on_texts(list(data.question1.values) +\n",
    "#                 list(data.question2.values.astype(str)))\n",
    "######################################################\n",
    "# When using tokenizer made earlier, use below\n",
    "######################################################\n",
    "try:\n",
    "    import cPickle as pickle # If in python 2 environment\n",
    "except:\n",
    "    import pickle # If in a pythnon 3 environment\n",
    "\n",
    "tk = pickle.load(open('../data/tokenizer.p','rb'))\n",
    "\n",
    "word_index = tk.word_index\n",
    "######################################################\n",
    "\n",
    "x1 = tk.texts_to_sequences(data.question1.values)\n",
    "x1 = sequence.pad_sequences(x1,maxlen=max_len)\n",
    "\n",
    "x2 = tk.texts_to_sequences(data.question2.values.astype(str))\n",
    "x2 = sequence.pad_sequences(x2,maxlen=max_len)\n",
    "\n",
    "ytrain_enc = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1889it [00:00, 18886.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:51, 19763.79it/s]\n",
      "  9%|▉         | 8622/95603 [00:00<00:01, 86212.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n",
      "Creating embedding matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95603/95603 [00:00<00:00, 122773.08it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Generating embeddings...')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(data_loc + 'glove.840B.300d.txt')\n",
    "unfound = []\n",
    "unfound_vals = []\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        unfound.append(word)\n",
    "        unfound_vals.append(values[1:])\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "print('Creating embedding matrix...')\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "max_features = 200000\n",
    "filter_length = 5\n",
    "nb_filter = 64\n",
    "pool_length = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "\n",
    "\n",
    "# Question 1 - Embeddings -> Convolutional\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=40,\n",
    "                     trainable=False))\n",
    "model3.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "# model3.add(Dropout(0.2))\n",
    "\n",
    "# model3.add(Convolution1D(nb_filter=nb_filter,\n",
    "#                          filter_length=filter_length,\n",
    "#                          border_mode='valid',\n",
    "#                          activation='relu',\n",
    "#                          subsample_length=1))\n",
    "\n",
    "model3.add(GlobalMaxPooling1D())\n",
    "# model3.add(Dropout(0.2))\n",
    "\n",
    "# model3.add(Dense(300))\n",
    "# model3.add(Dropout(0.2))\n",
    "# model3.add(BatchNormalization())\n",
    "\n",
    "\n",
    "\n",
    "# Question 2 - Embeddings -> Convolutional\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=40,\n",
    "                     trainable=False))\n",
    "model4.add(Convolution1D(nb_filter=nb_filter,\n",
    "                         filter_length=filter_length,\n",
    "                         border_mode='valid',\n",
    "                         activation='relu',\n",
    "                         subsample_length=1))\n",
    "# model4.add(Dropout(0.2))\n",
    "\n",
    "# model4.add(Convolution1D(nb_filter=nb_filter,\n",
    "#                          filter_length=filter_length,\n",
    "#                          border_mode='valid',\n",
    "#                          activation='relu',\n",
    "#                          subsample_length=1))\n",
    "\n",
    "model4.add(GlobalMaxPooling1D())\n",
    "# model4.add(Dropout(0.2))\n",
    "\n",
    "# model4.add(Dense(300))\n",
    "# model4.add(Dropout(0.2))\n",
    "# model4.add(BatchNormalization())\n",
    "\n",
    "# Merge all models - MERGE ALL QUESTIONS\n",
    "\n",
    "merged_model = Sequential()\n",
    "merged_model.add(Merge([model3, model4], mode='concat'))\n",
    "merged_model.add(BatchNormalization())\n",
    "\n",
    "# Feed Forward Network\n",
    "# merged_model.add(Dense(300))\n",
    "# merged_model.add(PReLU())\n",
    "# merged_model.add(Dropout(0.2))\n",
    "# merged_model.add(BatchNormalization())\n",
    "#\n",
    "# merged_model.add(Dense(300))\n",
    "# merged_model.add(PReLU())\n",
    "# merged_model.add(Dropout(0.2))\n",
    "# merged_model.add(BatchNormalization())\n",
    "#\n",
    "# merged_model.add(Dense(300))\n",
    "# merged_model.add(PReLU())\n",
    "# merged_model.add(Dropout(0.2))\n",
    "# merged_model.add(BatchNormalization())\n",
    "\n",
    "# Final node gives binary output\n",
    "merged_model.add(Dense(1))\n",
    "merged_model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loss, Optimizer, Accuracy\n",
    "merged_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 384\n",
    "# Save checkpoints\n",
    "checkpoint = ModelCheckpoint(output+'weights.h5', monitor='val_acc', save_best_only=True, verbose=2)\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint_board = Tensorboard(log_dir=output+'/logs/',batch_size=batch_size)\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "max_epoch_wait = 4\n",
    "checkpoint_3 = EarlyStopping(monitor='val_acc',patience=max_epoch_wait) # Mode is inferred by acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-54823ca3ead1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train model on x1 and x2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m merged_model.fit([x1, x2], y=y, batch_size=batch_size, nb_epoch=num_epochs,\n\u001b[0m\u001b[1;32m      5\u001b[0m                  verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint]) # Add tensorboard in callbacks list\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merged_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "\n",
    "# Train model on x1 and x2\n",
    "merged_model.fit([x1, x2], y=y, batch_size=batch_size, nb_epoch=num_epochs,\n",
    "                 verbose=1, validation_split=0.1, shuffle=True, callbacks=[checkpoint]) # Add tensorboard in callbacks list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Model\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = merged_model.to_json()\n",
    "with open(output + \"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on submission data\n",
    "\n",
    "# Import test data\n",
    "csv_test = data_loc + 'test.csv'\n",
    "test_data = pd.read_csv(csv_test)\n",
    "\n",
    "t_x1 = tk.texts_to_sequences(test_data.question1.values.astype(str))\n",
    "t_x1 = sequence.pad_sequences(t_x1,maxlen=max_len)\n",
    "\n",
    "t_x2 = tk.texts_to_sequences(test_data.question2.values.astype(str))\n",
    "t_x2 = sequence.pad_sequences(t_x2,maxlen=max_len)\n",
    "\n",
    "p_test = loaded_model.predict([t_x1,t_x2],batch_size=100,verbose=1)\n",
    "\n",
    "# Predict\n",
    "df_test = pd.read_csv(data_loc+'test.csv')\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "sub.to_csv(output + 'predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
